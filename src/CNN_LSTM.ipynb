{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines (label = True): 1250000\n",
      "10000 lines processed\n",
      "20000 lines processed\n",
      "30000 lines processed\n",
      "40000 lines processed\n",
      "50000 lines processed\n",
      "60000 lines processed\n",
      "70000 lines processed\n",
      "80000 lines processed\n",
      "90000 lines processed\n",
      "100000 lines processed\n",
      "110000 lines processed\n",
      "120000 lines processed\n",
      "130000 lines processed\n",
      "140000 lines processed\n",
      "150000 lines processed\n",
      "160000 lines processed\n",
      "170000 lines processed\n",
      "180000 lines processed\n",
      "190000 lines processed\n",
      "200000 lines processed\n",
      "210000 lines processed\n",
      "220000 lines processed\n",
      "230000 lines processed\n",
      "240000 lines processed\n",
      "250000 lines processed\n",
      "260000 lines processed\n",
      "270000 lines processed\n",
      "280000 lines processed\n",
      "290000 lines processed\n",
      "300000 lines processed\n",
      "310000 lines processed\n",
      "320000 lines processed\n",
      "330000 lines processed\n",
      "340000 lines processed\n",
      "350000 lines processed\n",
      "360000 lines processed\n",
      "370000 lines processed\n",
      "380000 lines processed\n",
      "390000 lines processed\n",
      "400000 lines processed\n",
      "410000 lines processed\n",
      "420000 lines processed\n",
      "430000 lines processed\n",
      "440000 lines processed\n",
      "450000 lines processed\n",
      "460000 lines processed\n",
      "470000 lines processed\n",
      "480000 lines processed\n",
      "490000 lines processed\n",
      "500000 lines processed\n",
      "510000 lines processed\n",
      "520000 lines processed\n",
      "530000 lines processed\n",
      "540000 lines processed\n",
      "550000 lines processed\n",
      "560000 lines processed\n",
      "570000 lines processed\n",
      "580000 lines processed\n",
      "590000 lines processed\n",
      "600000 lines processed\n",
      "610000 lines processed\n",
      "620000 lines processed\n",
      "630000 lines processed\n",
      "640000 lines processed\n",
      "650000 lines processed\n",
      "660000 lines processed\n",
      "670000 lines processed\n",
      "680000 lines processed\n",
      "690000 lines processed\n",
      "700000 lines processed\n",
      "710000 lines processed\n",
      "720000 lines processed\n",
      "730000 lines processed\n",
      "740000 lines processed\n",
      "750000 lines processed\n",
      "760000 lines processed\n",
      "770000 lines processed\n",
      "780000 lines processed\n",
      "790000 lines processed\n",
      "800000 lines processed\n",
      "810000 lines processed\n",
      "820000 lines processed\n",
      "830000 lines processed\n",
      "840000 lines processed\n",
      "850000 lines processed\n",
      "860000 lines processed\n",
      "870000 lines processed\n",
      "880000 lines processed\n",
      "890000 lines processed\n",
      "900000 lines processed\n",
      "910000 lines processed\n",
      "920000 lines processed\n",
      "930000 lines processed\n",
      "940000 lines processed\n",
      "950000 lines processed\n",
      "960000 lines processed\n",
      "970000 lines processed\n",
      "980000 lines processed\n",
      "990000 lines processed\n",
      "1000000 lines processed\n",
      "1010000 lines processed\n",
      "1020000 lines processed\n",
      "1030000 lines processed\n",
      "1040000 lines processed\n",
      "1050000 lines processed\n",
      "1060000 lines processed\n",
      "1070000 lines processed\n",
      "1080000 lines processed\n",
      "1090000 lines processed\n",
      "1100000 lines processed\n",
      "1110000 lines processed\n",
      "1120000 lines processed\n",
      "1130000 lines processed\n",
      "1140000 lines processed\n",
      "1150000 lines processed\n",
      "1160000 lines processed\n",
      "1170000 lines processed\n",
      "1180000 lines processed\n",
      "1190000 lines processed\n",
      "1200000 lines processed\n",
      "1210000 lines processed\n",
      "1220000 lines processed\n",
      "1230000 lines processed\n",
      "1240000 lines processed\n",
      "1250000 lines processed\n",
      "Total lines (label = False): 1250000\n",
      "10000 lines processed\n",
      "20000 lines processed\n",
      "30000 lines processed\n",
      "40000 lines processed\n",
      "50000 lines processed\n",
      "60000 lines processed\n",
      "70000 lines processed\n",
      "80000 lines processed\n",
      "90000 lines processed\n",
      "100000 lines processed\n",
      "110000 lines processed\n",
      "120000 lines processed\n",
      "130000 lines processed\n",
      "140000 lines processed\n",
      "150000 lines processed\n",
      "160000 lines processed\n",
      "170000 lines processed\n",
      "180000 lines processed\n",
      "190000 lines processed\n",
      "200000 lines processed\n",
      "210000 lines processed\n",
      "220000 lines processed\n",
      "230000 lines processed\n",
      "240000 lines processed\n",
      "250000 lines processed\n",
      "260000 lines processed\n",
      "270000 lines processed\n",
      "280000 lines processed\n",
      "290000 lines processed\n",
      "300000 lines processed\n",
      "310000 lines processed\n",
      "320000 lines processed\n",
      "330000 lines processed\n",
      "340000 lines processed\n",
      "350000 lines processed\n",
      "360000 lines processed\n",
      "370000 lines processed\n",
      "380000 lines processed\n",
      "390000 lines processed\n",
      "400000 lines processed\n",
      "410000 lines processed\n",
      "420000 lines processed\n",
      "430000 lines processed\n",
      "440000 lines processed\n",
      "450000 lines processed\n",
      "460000 lines processed\n",
      "470000 lines processed\n",
      "480000 lines processed\n",
      "490000 lines processed\n",
      "500000 lines processed\n",
      "510000 lines processed\n",
      "520000 lines processed\n",
      "530000 lines processed\n",
      "540000 lines processed\n",
      "550000 lines processed\n",
      "560000 lines processed\n",
      "570000 lines processed\n",
      "580000 lines processed\n",
      "590000 lines processed\n",
      "600000 lines processed\n",
      "610000 lines processed\n",
      "620000 lines processed\n",
      "630000 lines processed\n",
      "640000 lines processed\n",
      "650000 lines processed\n",
      "660000 lines processed\n",
      "670000 lines processed\n",
      "680000 lines processed\n",
      "690000 lines processed\n",
      "700000 lines processed\n",
      "710000 lines processed\n",
      "720000 lines processed\n",
      "730000 lines processed\n",
      "740000 lines processed\n",
      "750000 lines processed\n",
      "760000 lines processed\n",
      "770000 lines processed\n",
      "780000 lines processed\n",
      "790000 lines processed\n",
      "800000 lines processed\n",
      "810000 lines processed\n",
      "820000 lines processed\n",
      "830000 lines processed\n",
      "840000 lines processed\n",
      "850000 lines processed\n",
      "860000 lines processed\n",
      "870000 lines processed\n",
      "880000 lines processed\n",
      "890000 lines processed\n",
      "900000 lines processed\n",
      "910000 lines processed\n",
      "920000 lines processed\n",
      "930000 lines processed\n",
      "940000 lines processed\n",
      "950000 lines processed\n",
      "960000 lines processed\n",
      "970000 lines processed\n",
      "980000 lines processed\n",
      "990000 lines processed\n",
      "1000000 lines processed\n",
      "1010000 lines processed\n",
      "1020000 lines processed\n",
      "1030000 lines processed\n",
      "1040000 lines processed\n",
      "1050000 lines processed\n",
      "1060000 lines processed\n",
      "1070000 lines processed\n",
      "1080000 lines processed\n",
      "1090000 lines processed\n",
      "1100000 lines processed\n",
      "1110000 lines processed\n",
      "1120000 lines processed\n",
      "1130000 lines processed\n",
      "1140000 lines processed\n",
      "1150000 lines processed\n",
      "1160000 lines processed\n",
      "1170000 lines processed\n",
      "1180000 lines processed\n",
      "1190000 lines processed\n",
      "1200000 lines processed\n",
      "1210000 lines processed\n",
      "1220000 lines processed\n",
      "1230000 lines processed\n",
      "1240000 lines processed\n",
      "1250000 lines processed\n"
     ]
    }
   ],
   "source": [
    "delete_substr = ['#', '<user>', '<url>']\n",
    "dataset.create_count('../data/train_pos_full.txt', delete_substr, True,0)\n",
    "dataset.create_count('../data/train_neg_full.txt', delete_substr, False,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.pos_count=np.load('pos_count.npy')\n",
    "dataset.neg_count=np.load('neg_count.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('pos_count', dataset.pos_count)\n",
    "np.save('neg_count', dataset.neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2881218274111674"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.neg_count['believe']/dataset.pos_count['believe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_count=np.load('pos_count.npy')\n",
    "neg_count=np.load('neg_count.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN & LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from create_submission import create_csv_submission\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import Dataset as data\n",
    "from gensim.models import Word2Vec\n",
    "from Word2VecModel import Word2VecModel\n",
    "from nltk.corpus import stopwords\n",
    "from Dataset import DataSet\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D,Flatten\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2VecModel(vector_size=128, word_min_count=5)\n",
    "word2vec.load_model('model_word2vec_128.bin')\n",
    "dataset = DataSet(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines (): 1250000\n",
      "10000 lines processed  len words 4\n",
      "20000 lines processed  len words 3\n",
      "30000 lines processed  len words 4\n",
      "40000 lines processed  len words 2\n",
      "50000 lines processed  len words 11\n",
      "60000 lines processed  len words 6\n",
      "70000 lines processed  len words 10\n",
      "80000 lines processed  len words 1\n",
      "90000 lines processed  len words 9\n",
      "100000 lines processed  len words 10\n",
      "110000 lines processed  len words 5\n",
      "120000 lines processed  len words 3\n",
      "130000 lines processed  len words 18\n",
      "140000 lines processed  len words 8\n",
      "150000 lines processed  len words 10\n",
      "160000 lines processed  len words 11\n",
      "170000 lines processed  len words 5\n",
      "180000 lines processed  len words 2\n",
      "190000 lines processed  len words 3\n",
      "200000 lines processed  len words 1\n",
      "210000 lines processed  len words 6\n",
      "220000 lines processed  len words 6\n",
      "230000 lines processed  len words 4\n",
      "240000 lines processed  len words 10\n",
      "250000 lines processed  len words 2\n",
      "260000 lines processed  len words 8\n",
      "270000 lines processed  len words 4\n",
      "280000 lines processed  len words 7\n",
      "290000 lines processed  len words 5\n",
      "300000 lines processed  len words 6\n",
      "310000 lines processed  len words 5\n",
      "320000 lines processed  len words 13\n",
      "330000 lines processed  len words 8\n",
      "340000 lines processed  len words 7\n",
      "350000 lines processed  len words 2\n",
      "360000 lines processed  len words 6\n",
      "370000 lines processed  len words 3\n",
      "380000 lines processed  len words 6\n",
      "390000 lines processed  len words 10\n",
      "400000 lines processed  len words 3\n",
      "410000 lines processed  len words 6\n",
      "420000 lines processed  len words 7\n",
      "430000 lines processed  len words 17\n",
      "440000 lines processed  len words 10\n",
      "450000 lines processed  len words 4\n",
      "460000 lines processed  len words 4\n",
      "470000 lines processed  len words 5\n",
      "480000 lines processed  len words 10\n",
      "490000 lines processed  len words 7\n",
      "500000 lines processed  len words 5\n",
      "510000 lines processed  len words 3\n",
      "520000 lines processed  len words 2\n",
      "530000 lines processed  len words 6\n",
      "540000 lines processed  len words 6\n",
      "550000 lines processed  len words 8\n",
      "560000 lines processed  len words 2\n",
      "570000 lines processed  len words 4\n",
      "580000 lines processed  len words 1\n",
      "590000 lines processed  len words 1\n",
      "600000 lines processed  len words 3\n",
      "610000 lines processed  len words 7\n",
      "620000 lines processed  len words 2\n",
      "630000 lines processed  len words 1\n",
      "640000 lines processed  len words 1\n",
      "650000 lines processed  len words 1\n",
      "660000 lines processed  len words 15\n",
      "670000 lines processed  len words 3\n",
      "680000 lines processed  len words 12\n",
      "690000 lines processed  len words 4\n",
      "700000 lines processed  len words 5\n",
      "710000 lines processed  len words 1\n",
      "720000 lines processed  len words 9\n",
      "730000 lines processed  len words 7\n",
      "740000 lines processed  len words 7\n",
      "750000 lines processed  len words 4\n",
      "760000 lines processed  len words 5\n",
      "770000 lines processed  len words 7\n",
      "780000 lines processed  len words 6\n",
      "790000 lines processed  len words 4\n",
      "800000 lines processed  len words 11\n",
      "810000 lines processed  len words 5\n",
      "820000 lines processed  len words 5\n",
      "830000 lines processed  len words 6\n",
      "840000 lines processed  len words 7\n",
      "850000 lines processed  len words 2\n",
      "860000 lines processed  len words 13\n",
      "870000 lines processed  len words 11\n",
      "880000 lines processed  len words 7\n",
      "890000 lines processed  len words 5\n",
      "900000 lines processed  len words 3\n",
      "910000 lines processed  len words 3\n",
      "920000 lines processed  len words 4\n",
      "930000 lines processed  len words 5\n",
      "940000 lines processed  len words 11\n",
      "950000 lines processed  len words 7\n",
      "960000 lines processed  len words 3\n",
      "970000 lines processed  len words 4\n",
      "980000 lines processed  len words 6\n",
      "990000 lines processed  len words 3\n",
      "1000000 lines processed  len words 7\n",
      "1010000 lines processed  len words 3\n",
      "1020000 lines processed  len words 9\n",
      "1030000 lines processed  len words 3\n",
      "1040000 lines processed  len words 2\n",
      "1050000 lines processed  len words 6\n",
      "1060000 lines processed  len words 5\n",
      "1070000 lines processed  len words 3\n",
      "1080000 lines processed  len words 14\n",
      "1090000 lines processed  len words 3\n",
      "1100000 lines processed  len words 12\n",
      "1110000 lines processed  len words 6\n",
      "1120000 lines processed  len words 9\n",
      "1130000 lines processed  len words 6\n",
      "1140000 lines processed  len words 1\n",
      "1150000 lines processed  len words 7\n",
      "1160000 lines processed  len words 6\n",
      "1170000 lines processed  len words 8\n",
      "1180000 lines processed  len words 8\n",
      "1190000 lines processed  len words 4\n",
      "1200000 lines processed  len words 2\n",
      "1210000 lines processed  len words 7\n",
      "1220000 lines processed  len words 4\n",
      "1230000 lines processed  len words 6\n",
      "1240000 lines processed  len words 0\n",
      "1250000 lines processed  len words 4\n",
      "Total lines (): 1250000\n",
      "10000 lines processed  len words 5\n",
      "20000 lines processed  len words 12\n",
      "30000 lines processed  len words 14\n",
      "40000 lines processed  len words 10\n",
      "50000 lines processed  len words 5\n",
      "60000 lines processed  len words 13\n",
      "70000 lines processed  len words 11\n",
      "80000 lines processed  len words 5\n",
      "90000 lines processed  len words 7\n",
      "100000 lines processed  len words 4\n",
      "110000 lines processed  len words 3\n",
      "120000 lines processed  len words 11\n",
      "130000 lines processed  len words 7\n",
      "140000 lines processed  len words 14\n",
      "150000 lines processed  len words 6\n",
      "160000 lines processed  len words 13\n",
      "170000 lines processed  len words 2\n",
      "180000 lines processed  len words 5\n",
      "190000 lines processed  len words 14\n",
      "200000 lines processed  len words 6\n",
      "210000 lines processed  len words 12\n",
      "220000 lines processed  len words 3\n",
      "230000 lines processed  len words 2\n",
      "240000 lines processed  len words 14\n",
      "250000 lines processed  len words 5\n",
      "260000 lines processed  len words 11\n",
      "270000 lines processed  len words 6\n",
      "280000 lines processed  len words 13\n",
      "290000 lines processed  len words 6\n",
      "300000 lines processed  len words 10\n",
      "310000 lines processed  len words 5\n",
      "320000 lines processed  len words 7\n",
      "330000 lines processed  len words 6\n",
      "340000 lines processed  len words 9\n",
      "350000 lines processed  len words 13\n",
      "360000 lines processed  len words 10\n",
      "370000 lines processed  len words 13\n",
      "380000 lines processed  len words 4\n",
      "390000 lines processed  len words 7\n",
      "400000 lines processed  len words 7\n",
      "410000 lines processed  len words 6\n",
      "420000 lines processed  len words 8\n",
      "430000 lines processed  len words 1\n",
      "440000 lines processed  len words 4\n",
      "450000 lines processed  len words 9\n",
      "460000 lines processed  len words 6\n",
      "470000 lines processed  len words 10\n",
      "480000 lines processed  len words 11\n",
      "490000 lines processed  len words 2\n",
      "500000 lines processed  len words 9\n",
      "510000 lines processed  len words 12\n",
      "520000 lines processed  len words 9\n",
      "530000 lines processed  len words 13\n",
      "540000 lines processed  len words 12\n",
      "550000 lines processed  len words 12\n",
      "560000 lines processed  len words 8\n",
      "570000 lines processed  len words 14\n",
      "580000 lines processed  len words 6\n",
      "590000 lines processed  len words 4\n",
      "600000 lines processed  len words 15\n",
      "610000 lines processed  len words 4\n",
      "620000 lines processed  len words 15\n",
      "630000 lines processed  len words 14\n",
      "640000 lines processed  len words 14\n",
      "650000 lines processed  len words 13\n",
      "660000 lines processed  len words 8\n",
      "670000 lines processed  len words 14\n",
      "680000 lines processed  len words 3\n",
      "690000 lines processed  len words 5\n",
      "700000 lines processed  len words 13\n",
      "710000 lines processed  len words 4\n",
      "720000 lines processed  len words 12\n",
      "730000 lines processed  len words 2\n",
      "740000 lines processed  len words 10\n",
      "750000 lines processed  len words 13\n",
      "760000 lines processed  len words 9\n",
      "770000 lines processed  len words 12\n",
      "780000 lines processed  len words 5\n",
      "790000 lines processed  len words 3\n",
      "800000 lines processed  len words 10\n",
      "810000 lines processed  len words 1\n",
      "820000 lines processed  len words 13\n",
      "830000 lines processed  len words 10\n",
      "840000 lines processed  len words 5\n",
      "850000 lines processed  len words 11\n",
      "860000 lines processed  len words 14\n",
      "870000 lines processed  len words 8\n",
      "880000 lines processed  len words 11\n",
      "890000 lines processed  len words 2\n",
      "900000 lines processed  len words 6\n",
      "910000 lines processed  len words 12\n",
      "920000 lines processed  len words 15\n",
      "930000 lines processed  len words 13\n",
      "940000 lines processed  len words 3\n",
      "950000 lines processed  len words 9\n",
      "960000 lines processed  len words 13\n",
      "970000 lines processed  len words 5\n",
      "980000 lines processed  len words 7\n",
      "990000 lines processed  len words 14\n",
      "1000000 lines processed  len words 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010000 lines processed  len words 11\n",
      "1020000 lines processed  len words 7\n",
      "1030000 lines processed  len words 14\n",
      "1040000 lines processed  len words 4\n",
      "1050000 lines processed  len words 5\n",
      "1060000 lines processed  len words 5\n",
      "1070000 lines processed  len words 10\n",
      "1080000 lines processed  len words 5\n",
      "1090000 lines processed  len words 3\n",
      "1100000 lines processed  len words 10\n",
      "1110000 lines processed  len words 6\n",
      "1120000 lines processed  len words 11\n",
      "1130000 lines processed  len words 3\n",
      "1140000 lines processed  len words 6\n",
      "1150000 lines processed  len words 14\n",
      "1160000 lines processed  len words 10\n",
      "1170000 lines processed  len words 8\n",
      "1180000 lines processed  len words 11\n",
      "1190000 lines processed  len words 15\n",
      "1200000 lines processed  len words 13\n",
      "1210000 lines processed  len words 5\n",
      "1220000 lines processed  len words 5\n",
      "1230000 lines processed  len words 7\n",
      "1240000 lines processed  len words 9\n",
      "1250000 lines processed  len words 13\n",
      "X_train shape: (2490064,)\n",
      "y_train shape: (2490064,)\n"
     ]
    }
   ],
   "source": [
    "deleted_substr = ['#', '<user>', '<url>']\n",
    "dataset.create_train_test('../data/train_pos_full.txt', '../data/train_neg_full.txt', delete_substr=deleted_substr,\\\n",
    "                         training=True, cnn=True,weight=0, stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.X_train_cnn = np.load('X_train_cnn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.y_train_cnn = np.load('y_train_cnn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel',\n",
       " 'rollercoaster',\n",
       " 'life',\n",
       " 'song',\n",
       " 'life',\n",
       " 'yolo',\n",
       " 'becoming',\n",
       " 'famous',\n",
       " 'x']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X_train_cnn[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines (): 10000\n",
      "10000 lines processed  len words 16\n",
      "X_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "dataset.X_test_cnn=[]\n",
    "deleted_substr = ['#', '<user>', '<url>']\n",
    "dataset.create_train_test('../data/test_data.txt', '', delete_substr=deleted_substr, training=False, \\\n",
    "                           cnn=True,weight=0, stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.X_test_cnn = np.load('X_test_cnn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[len(line) for line in dataset.X_train_cnn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 16672\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, embedding_layer = dataset.create_embedding(embedding_dim=128,seq_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, dataset.y_train_cnn, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 15, 128)      12622080    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 13, 128)      49280       embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 12, 128)      65664       embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 13, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 12, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 13, 128)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 12, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 13, 128)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 12, 128)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           8224        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32)           128         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            33          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,746,433\n",
      "Trainable params: 123,777\n",
      "Non-trainable params: 12,622,656\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 128\n",
    "\n",
    "inp = Input(shape=(X_train.shape[1],))\n",
    "emb = embedding_layer(inp)\n",
    "\n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3)(emb)\n",
    "btch1_1 = BatchNormalization()(conv1_1)\n",
    "drp1_1  = Dropout(0.2)(btch1_1)\n",
    "actv1_1 = Activation('relu')(drp1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=4)(emb)\n",
    "btch1_2 = BatchNormalization()(conv1_2)\n",
    "drp1_2  = Dropout(0.2)(btch1_2)\n",
    "actv1_2 = Activation('relu')(drp1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2], axis=1)\n",
    "drp1 = Dropout(0.2)(cnct)\n",
    "\n",
    "dns1  = Dense(32, activation='relu')(drp1)\n",
    "btch1 = BatchNormalization()(dns1)\n",
    "drp2  = Dropout(0.2)(btch1)\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(drp2)\n",
    "model2 = Model(inputs=inp, outputs=out)\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1793242 samples, validate on 199250 samples\n",
      "Epoch 1/4\n",
      "1793242/1793242 [==============================] - 306s 170us/step - loss: 0.4416 - acc: 0.7848 - val_loss: 0.4128 - val_acc: 0.8047\n",
      "Epoch 2/4\n",
      " 384640/1793242 [=====>........................] - ETA: 4:03 - loss: 0.4215 - acc: 0.7985"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2d35ef5b6368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.fit(X_train, y_train,  validation_split=0.1, epochs=4, shuffle=True, verbose=1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a86228a20>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(embedding_layer)\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=4))\n",
    "model_conv.add(LSTM(100))\n",
    "model_conv.add(Dense(1, activation='sigmoid'))\n",
    "model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 128)           12622080  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 36, 64)            41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 9, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 12,729,205\n",
      "Trainable params: 107,125\n",
      "Non-trainable params: 12,622,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1792845 samples, validate on 199206 samples\n",
      "Epoch 1/4\n",
      "1792845/1792845 [==============================] - 383s 214us/step - loss: 0.4363 - acc: 0.7863 - val_loss: 0.4127 - val_acc: 0.8015\n",
      "Epoch 2/4\n",
      "  44160/1792845 [..............................] - ETA: 6:30 - loss: 0.4197 - acc: 0.7956"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-abff8d333a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(X_train, y_train,  validation_split=0.1, epochs=4, shuffle=True, verbose=1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "layer = embedding_layer(inputs)\n",
    "layer = LSTM(64)(layer)\n",
    "layer = Dense(1,name='out_layer')(layer)\n",
    "layer = Activation('sigmoid')(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1792845 samples, validate on 199206 samples\n",
      "Epoch 1/4\n",
      "1792845/1792845 [==============================] - 530s 295us/step - loss: 0.4037 - acc: 0.8079 - val_loss: 0.3914 - val_acc: 0.8154\n",
      "Epoch 2/4\n",
      "1792845/1792845 [==============================] - 594s 331us/step - loss: 0.3826 - acc: 0.8209 - val_loss: 0.3858 - val_acc: 0.8192\n",
      "Epoch 3/4\n",
      "1792845/1792845 [==============================] - 527s 294us/step - loss: 0.3753 - acc: 0.8252 - val_loss: 0.3828 - val_acc: 0.8213\n",
      "Epoch 4/4\n",
      "1792845/1792845 [==============================] - 518s 289us/step - loss: 0.3705 - acc: 0.8277 - val_loss: 0.3811 - val_acc: 0.8216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a46d95dd8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,  validation_split=0.1, epochs=4, shuffle=True, verbose=1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1792845 samples, validate on 199206 samples\n",
      "Epoch 1/4\n",
      "1792845/1792845 [==============================] - 507s 283us/step - loss: 0.3684 - acc: 0.8291 - val_loss: 0.3813 - val_acc: 0.8225\n",
      "Epoch 2/4\n",
      "1792845/1792845 [==============================] - 508s 283us/step - loss: 0.3659 - acc: 0.8303 - val_loss: 0.3798 - val_acc: 0.8233\n",
      "Epoch 3/4\n",
      "1792845/1792845 [==============================] - 509s 284us/step - loss: 0.3638 - acc: 0.8316 - val_loss: 0.3797 - val_acc: 0.8231\n",
      "Epoch 4/4\n",
      "1792845/1792845 [==============================] - 510s 285us/step - loss: 0.3617 - acc: 0.8326 - val_loss: 0.3793 - val_acc: 0.8231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a46ac8f28>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,  validation_split=0.1, epochs=4, shuffle=True, verbose=1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 128)           12622080  \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 36, 250)           160250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 12,845,331\n",
      "Trainable params: 223,251\n",
      "Non-trainable params: 12,622,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(embedding_layer)\n",
    "model_1.add(Conv1D(250,5,padding='valid',activation='relu',strides=1))\n",
    "model_1.add(GlobalMaxPooling1D())\n",
    "model_1.add(Dense(250))\n",
    "#model_1.add(Dropout(0.4))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dense(1))\n",
    "model_1.add(Activation('sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1792845 samples, validate on 199206 samples\n",
      "Epoch 1/4\n",
      "1638784/1792845 [==========================>...] - ETA: 58s - loss: 0.4178 - acc: 0.7989"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-81f82da93083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_1.fit(X_train, y_train,  validation_split=0.1, epochs=4, shuffle=True, verbose=1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1793242 samples, validate on 199250 samples\n",
      "Epoch 1/4\n",
      "1793242/1793242 [==============================] - 225s 125us/step - loss: 0.4219 - acc: 0.7965 - val_loss: 0.4032 - val_acc: 0.8076\n",
      "Epoch 2/4\n",
      " 989056/1793242 [===============>..............] - ETA: 1:35 - loss: 0.3960 - acc: 0.8124"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-81f82da93083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gunesyurdakul/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_1.fit(X_train, y_train,  validation_split=0.1, epochs=4, shuffle=True, verbose=1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
