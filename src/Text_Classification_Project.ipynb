{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPFL Machine Learning: Text Classification Project\n",
    "\n",
    "## Part 1: Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import DataSet\n",
    "from GloveModel import GloveModel\n",
    "from Word2VecModel import Word2VecModel\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "word_min_count = 5\n",
    "word2vec = Word2VecModel(embedding_size, word_min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_substr = ['#', '<user>', '<url>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding negative tweets\n",
      "Number of tweets: 1250000\n",
      "10000 lines processed\n",
      "20000 lines processed\n",
      "30000 lines processed\n",
      "40000 lines processed\n",
      "50000 lines processed\n",
      "60000 lines processed\n",
      "70000 lines processed\n",
      "80000 lines processed\n",
      "90000 lines processed\n",
      "100000 lines processed\n",
      "110000 lines processed\n",
      "120000 lines processed\n",
      "130000 lines processed\n",
      "140000 lines processed\n",
      "150000 lines processed\n",
      "160000 lines processed\n",
      "170000 lines processed\n",
      "180000 lines processed\n",
      "190000 lines processed\n",
      "200000 lines processed\n",
      "210000 lines processed\n",
      "220000 lines processed\n",
      "230000 lines processed\n",
      "240000 lines processed\n",
      "250000 lines processed\n",
      "260000 lines processed\n",
      "270000 lines processed\n",
      "280000 lines processed\n",
      "290000 lines processed\n",
      "300000 lines processed\n",
      "310000 lines processed\n",
      "320000 lines processed\n",
      "330000 lines processed\n",
      "340000 lines processed\n",
      "350000 lines processed\n",
      "360000 lines processed\n",
      "370000 lines processed\n",
      "380000 lines processed\n",
      "390000 lines processed\n",
      "400000 lines processed\n",
      "410000 lines processed\n",
      "420000 lines processed\n",
      "430000 lines processed\n",
      "440000 lines processed\n",
      "450000 lines processed\n",
      "460000 lines processed\n",
      "470000 lines processed\n",
      "480000 lines processed\n",
      "490000 lines processed\n",
      "500000 lines processed\n",
      "510000 lines processed\n",
      "520000 lines processed\n",
      "530000 lines processed\n",
      "540000 lines processed\n",
      "550000 lines processed\n",
      "560000 lines processed\n",
      "570000 lines processed\n",
      "580000 lines processed\n",
      "590000 lines processed\n",
      "600000 lines processed\n",
      "610000 lines processed\n",
      "620000 lines processed\n",
      "630000 lines processed\n",
      "640000 lines processed\n",
      "650000 lines processed\n",
      "660000 lines processed\n",
      "670000 lines processed\n",
      "680000 lines processed\n",
      "690000 lines processed\n",
      "700000 lines processed\n",
      "710000 lines processed\n",
      "720000 lines processed\n",
      "730000 lines processed\n",
      "740000 lines processed\n",
      "750000 lines processed\n",
      "760000 lines processed\n",
      "770000 lines processed\n",
      "780000 lines processed\n",
      "790000 lines processed\n",
      "800000 lines processed\n",
      "810000 lines processed\n",
      "820000 lines processed\n",
      "830000 lines processed\n",
      "840000 lines processed\n",
      "850000 lines processed\n",
      "860000 lines processed\n",
      "870000 lines processed\n",
      "880000 lines processed\n",
      "890000 lines processed\n",
      "900000 lines processed\n",
      "910000 lines processed\n",
      "920000 lines processed\n",
      "930000 lines processed\n",
      "940000 lines processed\n",
      "950000 lines processed\n",
      "960000 lines processed\n",
      "970000 lines processed\n",
      "980000 lines processed\n",
      "990000 lines processed\n",
      "1000000 lines processed\n",
      "1010000 lines processed\n",
      "1020000 lines processed\n",
      "1030000 lines processed\n",
      "1040000 lines processed\n",
      "1050000 lines processed\n",
      "1060000 lines processed\n",
      "1070000 lines processed\n",
      "1080000 lines processed\n",
      "1090000 lines processed\n",
      "1100000 lines processed\n",
      "1110000 lines processed\n",
      "1120000 lines processed\n",
      "1130000 lines processed\n",
      "1140000 lines processed\n",
      "1150000 lines processed\n",
      "1160000 lines processed\n",
      "1170000 lines processed\n",
      "1180000 lines processed\n",
      "1190000 lines processed\n",
      "1200000 lines processed\n",
      "1210000 lines processed\n",
      "1220000 lines processed\n",
      "1230000 lines processed\n",
      "1240000 lines processed\n",
      "1250000 lines processed\n",
      "Adding positive tweets\n",
      "Number of tweets: 1250000\n",
      "10000 lines processed\n",
      "20000 lines processed\n",
      "30000 lines processed\n",
      "40000 lines processed\n",
      "50000 lines processed\n",
      "60000 lines processed\n",
      "70000 lines processed\n",
      "80000 lines processed\n",
      "90000 lines processed\n",
      "100000 lines processed\n",
      "110000 lines processed\n",
      "120000 lines processed\n",
      "130000 lines processed\n",
      "140000 lines processed\n",
      "150000 lines processed\n",
      "160000 lines processed\n",
      "170000 lines processed\n",
      "180000 lines processed\n",
      "190000 lines processed\n",
      "200000 lines processed\n",
      "210000 lines processed\n",
      "220000 lines processed\n",
      "230000 lines processed\n",
      "240000 lines processed\n",
      "250000 lines processed\n",
      "260000 lines processed\n",
      "270000 lines processed\n",
      "280000 lines processed\n",
      "290000 lines processed\n",
      "300000 lines processed\n",
      "310000 lines processed\n",
      "320000 lines processed\n",
      "330000 lines processed\n",
      "340000 lines processed\n",
      "350000 lines processed\n",
      "360000 lines processed\n",
      "370000 lines processed\n",
      "380000 lines processed\n",
      "390000 lines processed\n",
      "400000 lines processed\n",
      "410000 lines processed\n",
      "420000 lines processed\n",
      "430000 lines processed\n",
      "440000 lines processed\n",
      "450000 lines processed\n",
      "460000 lines processed\n",
      "470000 lines processed\n",
      "480000 lines processed\n",
      "490000 lines processed\n",
      "500000 lines processed\n",
      "510000 lines processed\n",
      "520000 lines processed\n",
      "530000 lines processed\n",
      "540000 lines processed\n",
      "550000 lines processed\n",
      "560000 lines processed\n",
      "570000 lines processed\n",
      "580000 lines processed\n",
      "590000 lines processed\n",
      "600000 lines processed\n",
      "610000 lines processed\n",
      "620000 lines processed\n",
      "630000 lines processed\n",
      "640000 lines processed\n",
      "650000 lines processed\n",
      "660000 lines processed\n",
      "670000 lines processed\n",
      "680000 lines processed\n",
      "690000 lines processed\n",
      "700000 lines processed\n",
      "710000 lines processed\n",
      "720000 lines processed\n",
      "730000 lines processed\n",
      "740000 lines processed\n",
      "750000 lines processed\n",
      "760000 lines processed\n",
      "770000 lines processed\n",
      "780000 lines processed\n",
      "790000 lines processed\n",
      "800000 lines processed\n",
      "810000 lines processed\n",
      "820000 lines processed\n",
      "830000 lines processed\n",
      "840000 lines processed\n",
      "850000 lines processed\n",
      "860000 lines processed\n",
      "870000 lines processed\n",
      "880000 lines processed\n",
      "890000 lines processed\n",
      "900000 lines processed\n",
      "910000 lines processed\n",
      "920000 lines processed\n",
      "930000 lines processed\n",
      "940000 lines processed\n",
      "950000 lines processed\n",
      "960000 lines processed\n",
      "970000 lines processed\n",
      "980000 lines processed\n",
      "990000 lines processed\n",
      "1000000 lines processed\n",
      "1010000 lines processed\n",
      "1020000 lines processed\n",
      "1030000 lines processed\n",
      "1040000 lines processed\n",
      "1050000 lines processed\n",
      "1060000 lines processed\n",
      "1070000 lines processed\n",
      "1080000 lines processed\n",
      "1090000 lines processed\n",
      "1100000 lines processed\n",
      "1110000 lines processed\n",
      "1120000 lines processed\n",
      "1130000 lines processed\n",
      "1140000 lines processed\n",
      "1150000 lines processed\n",
      "1160000 lines processed\n",
      "1170000 lines processed\n",
      "1180000 lines processed\n",
      "1190000 lines processed\n",
      "1200000 lines processed\n",
      "1210000 lines processed\n",
      "1220000 lines processed\n",
      "1230000 lines processed\n",
      "1240000 lines processed\n",
      "1250000 lines processed\n"
     ]
    }
   ],
   "source": [
    "word2vec.read_text('../data/train_pos_full.txt', '../data/train_neg_full.txt', deleted_substr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts\n",
      "Training ends\n",
      "Word2Vec Model saved\n"
     ]
    }
   ],
   "source": [
    "word2vec.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cooccurrence matrix\n",
      "46597044 nonzero entries\n",
      "using nmax = 100 , cooc.max() = 2599902\n",
      "initializing embeddings\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "glove = GloveModel('../helpers/cooc.pkl')\n",
    "glove.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = np.load('word_embeddings_glove.npy')\n",
    "\n",
    "with open('../helpers/vocab.pkl', 'rb') as file:\n",
    "    vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Word Embeddings\n",
    "\n",
    "### Glove \n",
    "- Embedding Size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67763872,  0.23661041, -0.4362365 , -0.86834238,  0.30773711,\n",
       "        0.1206173 ,  0.85236568, -0.06076359, -0.16476113,  0.20033804,\n",
       "       -0.25506169, -0.39971067, -0.05757342,  0.16431074, -0.44331673,\n",
       "       -0.72183871, -0.15329704,  0.19056749, -0.4245216 , -0.52767019])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings[vocab['turkey']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- Embedding Size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.79334044e-01, -4.00997698e-02, -1.89287409e-01,  1.10475945e+00,\n",
       "       -1.97836533e-01,  6.95252270e-02, -1.63925743e+00,  3.25736135e-01,\n",
       "        3.39787193e-02, -1.05910473e-01, -9.07549262e-01, -1.38510871e+00,\n",
       "        4.44844037e-01, -1.14802003e-01, -6.47119939e-01,  7.40318000e-01,\n",
       "       -2.04286929e-02, -6.16852641e-01,  1.04869679e-01, -1.68358400e-01,\n",
       "       -6.17551506e-01,  9.53497231e-01,  3.52581702e-02, -7.84268200e-01,\n",
       "        7.29105234e-01, -1.09275615e+00,  5.29174984e-01,  2.22794682e-01,\n",
       "        2.21627617e+00,  1.60692477e+00,  7.26028919e-01, -1.43865871e+00,\n",
       "       -2.76584655e-01,  2.19503224e-01, -4.53999877e-01, -2.55639404e-01,\n",
       "        8.92999396e-02,  4.28477764e-01,  2.19303638e-01,  1.08325231e+00,\n",
       "        1.38622239e-01, -2.40545526e-01, -3.11490029e-01, -7.81302974e-02,\n",
       "       -8.70230138e-01, -1.15198886e+00, -4.43359107e-01, -4.61849362e-01,\n",
       "        3.45617354e-01,  3.21980327e-01, -3.81252527e-01,  3.97922657e-02,\n",
       "       -1.00297667e-01, -2.21512526e-01,  3.94451827e-01,  1.07097518e+00,\n",
       "        9.37236547e-01,  8.75365674e-01, -1.82760343e-01,  6.16850555e-01,\n",
       "        2.33346596e-01,  3.85700502e-02,  1.78076653e-03, -5.44442117e-01,\n",
       "       -9.99287784e-01,  6.21598423e-01,  2.32628867e-01, -8.12106654e-02,\n",
       "       -5.71392834e-01,  1.29084277e+00,  7.23282933e-01,  1.14468098e+00,\n",
       "       -6.27650678e-01, -4.05896634e-01, -2.72429019e-01,  5.69653690e-01,\n",
       "       -4.15220201e-01, -1.38314438e+00, -3.25235367e-01, -3.48488599e-01,\n",
       "        1.73713326e+00, -8.96982029e-02, -4.33965981e-01,  2.31008679e-01,\n",
       "        2.79274225e-01, -1.26764929e+00, -9.13889587e-01, -1.11914587e+00,\n",
       "       -8.90785277e-01, -4.76657540e-01, -8.96592736e-01, -1.40910363e+00,\n",
       "        9.33620870e-01,  5.13364613e-01,  4.58745897e-01, -8.10509622e-01,\n",
       "        4.45735008e-01,  1.91103911e+00,  8.94853890e-01, -6.34691864e-02,\n",
       "       -4.55720484e-01, -1.20433974e+00, -2.23500937e-01,  2.85246342e-01,\n",
       "        5.92418276e-02,  1.19689465e+00,  8.16146731e-01,  1.21763432e+00,\n",
       "       -5.25524914e-01,  3.84521522e-02, -3.30228418e-01, -9.52271372e-02,\n",
       "        2.27769330e-01, -1.17859876e+00, -3.18982929e-01,  1.22133183e+00,\n",
       "       -3.68978024e-01,  1.08130783e-01,  9.35863078e-01,  6.81797326e-01,\n",
       "        4.04366314e-01,  2.23718956e-01,  3.47936243e-01,  4.88022208e-01,\n",
       "       -1.14550471e+00, -6.96856678e-01, -2.26075339e+00, -5.81407011e-01,\n",
       "        3.72385979e-01, -2.03680944e+00, -6.69010803e-02, -4.26119030e-01,\n",
       "        1.15673971e+00,  5.57774782e-01,  5.28751075e-01, -6.02284193e-01,\n",
       "        8.51594687e-01, -1.16570604e+00, -1.78229287e-01, -4.25182015e-01,\n",
       "       -1.90428466e-01, -7.27428257e-01,  5.26608109e-01,  1.03484213e+00,\n",
       "        4.24629360e-01, -1.24718082e+00, -4.95782048e-01, -2.43184090e-01,\n",
       "       -1.62740922e+00,  9.73082960e-01, -1.76153675e-01, -8.60980809e-01,\n",
       "       -4.16904181e-01, -4.26699400e-01, -1.29298127e+00,  5.86183846e-01,\n",
       "       -1.06266057e+00,  2.85971791e-01,  1.16641305e-01, -3.51699948e-01,\n",
       "       -4.54535812e-01, -9.06570077e-01,  2.71160871e-01,  5.84099650e-01,\n",
       "        1.27319467e+00,  5.39583981e-01, -1.28490674e+00,  1.39772499e+00,\n",
       "       -4.12371874e-01, -9.78074610e-01,  3.79355013e-01, -5.96552670e-01,\n",
       "        7.91419625e-01,  5.76848313e-02, -2.85822362e-01,  5.49228787e-01,\n",
       "        1.15477598e+00,  3.67958218e-01, -8.18351388e-01, -1.05111623e+00,\n",
       "       -1.51945278e-01, -2.66248465e-01, -6.83223367e-01,  4.66614783e-01,\n",
       "        7.17270613e-01, -5.95695198e-01, -9.07371759e-01, -2.44810969e-01,\n",
       "       -3.14341873e-01,  3.96191537e-01, -1.93693972e+00,  4.22556221e-01,\n",
       "        2.53898829e-01, -3.82312298e-01,  5.80962896e-01, -4.93848443e-01,\n",
       "       -1.18626642e+00, -4.66988504e-01,  9.14843917e-01, -5.72883487e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.model.wv['turkey']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
